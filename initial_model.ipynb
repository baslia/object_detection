{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:17:06.225027Z",
     "start_time": "2024-01-30T09:17:06.219849Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, DetrFeatureExtractor, DetrConfig\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert CSV to COCO format\n",
    "# We need to convert the CSV to a JSON format like that:\n",
    "# annotation{\n",
    "# \"id\" : int, \"image_id\" : int, \"category_id\" : int, \"segmentation\" : RLE or [polygon], \"area\" : float, \"bbox\" : [x,y,width,height], \"iscrowd\" : 0 or 1,\n",
    "# }\n",
    "# \n",
    "# categories[{\n",
    "# \"id\" : int, \"name\" : str, \"supercategory\" : str,\n",
    "# }]\n",
    "\n",
    "def convert_csv_to_coco(csv_path, json_path):\n",
    "  import csv\n",
    "  import json\n",
    "  from collections import defaultdict\n",
    "\n",
    "  # Create the categories\n",
    "  categories = []\n",
    "  categories.append({\"id\": 0, \"name\": \"background\", \"supercategory\": \"\"})\n",
    "  categories.append({\"id\": 1, \"name\": \"rebar\", \"supercategory\": \"\"})\n",
    "\n",
    "  # Create the annotations\n",
    "  annotations = []\n",
    "  image_id = 0\n",
    "  annotation_id = 0\n",
    "  with open(csv_path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # row contains the follwing information [image_name, xmin, ymin, xmax, ymax]\n",
    "        image_name = row[0]\n",
    "        xmin = row[1]\n",
    "        ymin = row[2]\n",
    "        xmax = row[3]\n",
    "        ymax = row[4]\n",
    "        area = (xmax -xmin) * (ymax - ymin)\n",
    "        # Extract the image id from the image name\n",
    "        image_id = int(image_name.split(\"_\")[0][1])\n",
    "        for i in range(1, 5):\n",
    "            if row[f\"Label {i}\"] == \"rebar\":\n",
    "                annotation_id += 1\n",
    "                annotations.append({\"id\": annotation_id, \"image_id\": image_id, \"category_id\": 1, \"segmentation\": [], \"area\": 0, \"bbox\": [int(row[f\"Left {i}\"]), int(row[f\"Top {i}\"]), int(row[f\"Width {i}\"]), int(row[f\"Height {i}\"])], \"iscrowd\": 0})\n",
    "  \n",
    "  # Create the images\n",
    "  images = []\n",
    "  image_id = 0\n",
    "  with open(csv_path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "      image_id += 1\n",
    "      images.append({\"id\": image_id, \"width\": 1280, \"height\": 720, \"file_name\": row[\"External ID\"], \"license\": 0, \"flickr_url\": \"\", \"coco_url\": \"\", \"date_captured\": \"\"})\n",
    "\n",
    "  # Create the JSON\n",
    "  data = {\"info\": {}, \"licenses\": {}, \"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
    "  with open(json_path, 'w') as outfile:\n",
    "    json.dump(data, outfile)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b05c3a23d245b1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the Data loader\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"annotations/100_percent_train.json\" if train else \"annotations/test.json\")\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:20:55.251477Z",
     "start_time": "2024-01-30T09:20:55.245460Z"
    }
   },
   "id": "e9d638d81d94caea",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m img_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRebarDSC\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m feature_extractor \u001B[38;5;241m=\u001B[39m DetrFeatureExtractor\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model)\n\u001B[0;32m----> 8\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mCocoDetection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mimg_folder\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_extractor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m CocoDetection(img_folder\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimg_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, feature_extractor\u001B[38;5;241m=\u001B[39mfeature_extractor, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[15], line 5\u001B[0m, in \u001B[0;36mCocoDetection.__init__\u001B[0;34m(self, img_folder, feature_extractor, train)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img_folder, feature_extractor, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m      4\u001B[0m     ann_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(img_folder, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mannotations/100_percent_train.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m train \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mannotations/test.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCocoDetection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimg_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mann_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_extractor \u001B[38;5;241m=\u001B[39m feature_extractor\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/site-packages/torchvision/datasets/coco.py:36\u001B[0m, in \u001B[0;36mCocoDetection.__init__\u001B[0;34m(self, root, annFile, transform, target_transform, transforms)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(root, transforms, transform, target_transform)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpycocotools\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcoco\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m COCO\n\u001B[0;32m---> 36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoco \u001B[38;5;241m=\u001B[39m \u001B[43mCOCO\u001B[49m\u001B[43m(\u001B[49m\u001B[43mannFile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28msorted\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoco\u001B[38;5;241m.\u001B[39mimgs\u001B[38;5;241m.\u001B[39mkeys()))\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/site-packages/pycocotools/coco.py:82\u001B[0m, in \u001B[0;36mCOCO.__init__\u001B[0;34m(self, annotation_file)\u001B[0m\n\u001B[1;32m     80\u001B[0m tic \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(annotation_file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 82\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(dataset)\u001B[38;5;241m==\u001B[39m\u001B[38;5;28mdict\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mannotation file format \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m not supported\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(dataset))\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDone (t=\u001B[39m\u001B[38;5;132;01m{:0.2f}\u001B[39;00m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(time\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;241m-\u001B[39m tic))\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/json/__init__.py:293\u001B[0m, in \u001B[0;36mload\u001B[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(fp, \u001B[38;5;241m*\u001B[39m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_float\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    275\u001B[0m         parse_int\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_constant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_pairs_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    276\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001B[39;00m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;124;03m    a JSON document) to a Python object.\u001B[39;00m\n\u001B[1;32m    278\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001B[39;00m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobject_hook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobject_hook\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_float\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_int\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_int\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_constant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_constant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobject_pairs_hook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobject_pairs_hook\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscan_once(s, idx)\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Set up pre-trained model\n",
    "pretrained_model = \"facebook/detr-resnet-50\"\n",
    "\n",
    "img_folder = \"RebarDSC\"\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(pretrained_model)\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=f'{img_folder}', feature_extractor=feature_extractor)\n",
    "val_dataset = CocoDetection(img_folder=f'{img_folder}', feature_extractor=feature_extractor, train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:24:51.956074Z",
     "start_time": "2024-01-30T09:24:51.622695Z"
    }
   },
   "id": "52685952899234c9",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# The model needs finetunning to detect rebar\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDetr\u001B[39;00m(\u001B[43mpl\u001B[49m\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[1;32m      4\u001B[0m      \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, lr, lr_backbone, weight_decay):\n\u001B[1;32m      5\u001B[0m          \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "# The model needs finetunning to detect rebar\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         self.model = DetrForObjectDetection.from_pretrained(pretrained_model, \n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "     \n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:13:31.552537Z",
     "start_time": "2024-01-30T09:13:31.528540Z"
    }
   },
   "id": "47e5f54eaf625a56",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetrForObjectDetection(\n",
      "  (model): DetrModel(\n",
      "    (backbone): DetrConvModel(\n",
      "      (conv_encoder): DetrConvEncoder(\n",
      "        (model): FeatureListNet(\n",
      "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (bn1): DetrFrozenBatchNorm2d()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "          (layer1): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer2): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer3): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer4): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (position_embedding): DetrSinePositionEmbedding()\n",
      "    )\n",
      "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (query_position_embeddings): Embedding(100, 256)\n",
      "    (encoder): DetrEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DetrEncoderLayer(\n",
      "          (self_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): ReLU()\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DetrDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DetrDecoderLayer(\n",
      "          (self_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (class_labels_classifier): Linear(in_features=256, out_features=92, bias=True)\n",
      "  (bbox_predictor): DetrMLPPredictionHead(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Replace the pre-trained head with a new one\n",
    "print(model)\n",
    "# model.class_embed = Linear(in_features=model.class_embed.in_features, out_features=num_labels+1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:03:11.813175Z",
     "start_time": "2024-01-30T09:03:11.807858Z"
    }
   },
   "id": "97d2f5817a4a8bf8",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DetrConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"relu\",\n  \"architectures\": [\n    \"DetrForObjectDetection\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"auxiliary_loss\": false,\n  \"backbone\": \"resnet50\",\n  \"backbone_config\": null,\n  \"bbox_cost\": 5,\n  \"bbox_loss_coefficient\": 5,\n  \"class_cost\": 1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 256,\n  \"decoder_attention_heads\": 8,\n  \"decoder_ffn_dim\": 2048,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"dice_loss_coefficient\": 1,\n  \"dilation\": false,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 8,\n  \"encoder_ffn_dim\": 2048,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_coefficient\": 0.1,\n  \"giou_cost\": 2,\n  \"giou_loss_coefficient\": 2,\n  \"init_std\": 0.02,\n  \"init_xavier_std\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"mask_loss_coefficient\": 1,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"detr\",\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 6,\n  \"num_queries\": 100,\n  \"position_embedding_type\": \"sine\",\n  \"scale_embedding\": false,\n  \"transformers_version\": \"4.35.2\",\n  \"use_pretrained_backbone\": true,\n  \"use_timm_backbone\": true\n}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T08:54:49.292634Z",
     "start_time": "2024-01-30T08:54:49.287349Z"
    }
   },
   "id": "26ce577fe9e94511",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3408d49f4ed3ffc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
